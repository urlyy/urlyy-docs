# 爬虫能给你带来什么
- 一个自动化的数据采集员。
- 可用于分析的数据集。
- 大量的知识信息。(如LLM的联网搜索)
- 可能的国家饭。😊


# 爬虫的两种流派
1. 浏览器自动化操作。代表有 PlayWright 和 Selenium。这是最直观的爬虫，让计算机模拟人在获取数据时的登录、点击跳转、基于页面元素的目标定位、数据提取。因为有成熟的第三方库，所以实现起来不算太难。优点是能模拟人能进行的复杂流程，比较直观，不需要针对加密、鉴权等进行业务代码层面的分析、逆向等，只要页面上能显示的数据，就都能获取到。缺点是不能获得页面上未显示的数据，而且网站页面修改可能导致爬虫无效，并且速度较慢，同时还需要反复的调试，将运行极快的脚本适应速度较慢的页面更新与渲染。
2. 抓取请求。代表有 Requests。在前后端分离盛行的现在非常好用，因为有能直接获取数据的后端接口，不需要在数据渲染到页面之后再解析页面的元素。不过由于界面也是个 HTML 文本的请求响应数据，所以对于 PHP 这种 SSR 的网站，也能结合入 BeautifulSoup 等第三方库进行HTML文本解析，并进行后续的元素定位和数据提取。好处是速度较快，对于没有反爬机制、有数据接口的数据爬取任务有较快的完成速度，同时兼容目标网站是前端渲染或后端渲染两种场景。缺点是如果接口有严密的反爬机制，则需要付出非常大的努力进行逆向，对爬虫开发人员的能力要求较高。


# Robots协议
Robots协议是网站出于安全和隐私考虑，防止搜索引擎抓取敏感信息而设置的。它仅仅是多年逐步形成完善的一个互联网行业规范，并不具有技术约束和法律效益，是所谓的`君子协议`。但Robots协议可以被认作是一个`爬取红线`。
查看方法：网站域名/robots.txt。如`https://www.bilibili.com/robots.txt`。
字段解释如下，值支持正则表达式。
```
User-agent: 爬虫的名称。
Disallow: 不允许爬虫访问的地址。
Allow: 允许爬虫访问的地址。
```
网站设置的 Robots 协议基本上是针对搜索引擎的，对于个人非盈利性质的数据采集和数据使用，完全可以视若无物。因此对于网站一定存在的敏感数据，需要网站开发人员通过鉴权、反爬等措施进行保护。

至于爬虫爬成铁窗泪，也不是 Robots 协议的问题，而是后续的数据不正当使用，让爬虫上升到了黑客的性质，侵害了网站所属组织和用户的利益。

> Robots协议在判案中也能起到一定作用，还是不要忽视。


# 实现爬虫前要注意什么
- 学习框架的文档。
- 有一定的 Web 基础，能熟练定位目标数据对应的请求或者 HTML 元素，并能从结构化文本中解析出所需数据。
- 学习相关法律法规，要注意：
    - 政府、科研单位不管有无声明都不要爬。
    - 涉及类似知识产权、版权等类型的数据不要爬。
    - 不要让爬虫影响目标网站的正常运行。
    - 涉及公民隐私的敏感数据(如密码)不要爬。
    - 不要以牟利、搞破坏、不正当竞争等目的爬取商业数据。
    - 爬取 Robots 协议禁止爬取或者有反爬机制保护的数据，还是存在一定的法律风险。
    - 爬到的数据也不要大肆传播，问就是仅用于个人和学习使用。


# 爬虫技术路线
- Requests、Playwright、BeautifulSoup、lxml等爬虫基本库。
- 正则表达式、CSS选择器、Xpath选择器。
- 了解 Cookie、User-Agent、Session、Token 等网络请求相关的重要内容。
- 并发、协程相关 API 以实现高性能并发爬取。
- 数据持久化，如 MySQL、MongoDB、ES等。
- 基于 CV+自动化操作 来通过验证码。
- 搭建和使用账号池、代理池。
- 基于 MQ 等实现分布式爬取。
- 一些加密算法、数据混淆算法。
- AI + 爬虫?

# 推荐资料
- 电子书：`Python3 网络爬虫开发实战`
- [Playwright主页](https://playwright.dev/python/docs/intro)
- [Requests主页](https://requests.readthedocs.io/projects/cn/zh-cn/latest/)
- [Scrapy主页](https://scrapy.org)
- [EasySpider仓库](https://github.com/NaiboWang/EasySpider)
- [明确越界网络爬虫行为的刑事处罚边界](https://www.spp.gov.cn/spp/llyj/202202/t20220215_544538.shtml)
- [爬取数据须遵规](https://www.spp.gov.cn/llyj/202202/t20220210_543998.shtml)
- [公司让爬Robots.txt声明了不允许爬的网站应该怎么办？- 知乎](https://www.zhihu.com/question/396958646)